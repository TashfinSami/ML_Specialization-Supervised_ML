{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T20:08:30.475474Z",
     "start_time": "2025-03-18T20:08:30.459070Z"
    }
   },
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=2)  # reduced display precision on numpy arrays"
   ],
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T20:08:33.018969Z",
     "start_time": "2025-03-18T20:08:33.011499Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def load_house_data():\n",
    "    data = np.loadtxt(\"./data/houses.txt\", delimiter=',', skiprows=1)\n",
    "    X = data[:,:4]\n",
    "    y = data[:,4]\n",
    "    return X, y"
   ],
   "id": "2d1630d93a154132",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T20:08:34.208607Z",
     "start_time": "2025-03-18T20:08:34.188077Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, y_train = load_house_data()",
   "id": "19b52de85fcfe014",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T20:08:36.248476Z",
     "start_time": "2025-03-18T20:08:36.230110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "\n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma\n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "#check our work\n",
    "#from sklearn.preprocessing import scale\n",
    "#scale(X_orig, axis=0, with_mean=True, with_std=True, copy=True)"
   ],
   "id": "15cbb48f206c681d",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T20:08:37.209266Z",
     "start_time": "2025-03-18T20:08:37.200752Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost(X, y, w, b):\n",
    "    \"\"\"\n",
    "    compute cost\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      cost (scalar): cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    cost = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b           #(n,)(n,) = scalar (see np.dot)\n",
    "        cost = cost + (f_wb_i - y[i])**2       #scalar\n",
    "    cost = cost / (2 * m)                      #scalar\n",
    "    return cost"
   ],
   "id": "b2c915aa6a8f43e8",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T20:08:43.997666Z",
     "start_time": "2025-03-18T20:08:43.988033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression\n",
    "    Args:\n",
    "      X (ndarray (m,n)): Data, m examples with n features\n",
    "      y (ndarray (m,)) : target values\n",
    "      w (ndarray (n,)) : model parameters\n",
    "      b (scalar)       : model parameter\n",
    "\n",
    "    Returns:\n",
    "      dj_dw (ndarray (n,)): The gradient of the cost w.r.t. the parameters w.\n",
    "      dj_db (scalar):       The gradient of the cost w.r.t. the parameter b.\n",
    "    \"\"\"\n",
    "    m,n = X.shape           #(number of examples, number of features)\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "\n",
    "    #for j in range(n):\n",
    "     #   for i in range(m):\n",
    "      #      err = (np.dot(X[i], w) + b) - y[i]\n",
    "       #     dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "    # b has to  be calculated separately\n",
    "    #for i in range(m):\n",
    "     #   err = (np.dot(X[i], w) + b) - y[i]\n",
    "      #  dj_db = dj_db + err\n",
    "\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "\n",
    "    return dj_db, dj_dw"
   ],
   "id": "4732d4072bb3177a",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T20:08:46.272437Z",
     "start_time": "2025-03-18T20:08:46.258259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters):\n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn w and b. Updates w and b by taking\n",
    "    num_iters gradient steps with learning rate alpha\n",
    "\n",
    "    Args:\n",
    "      X (ndarray (m,n))   : Data, m examples with n features\n",
    "      y (ndarray (m,))    : target values\n",
    "      w_in (ndarray (n,)) : initial model parameters\n",
    "      b_in (scalar)       : initial model parameter\n",
    "      cost_function       : function to compute cost\n",
    "      gradient_function   : function to compute the gradient\n",
    "      alpha (float)       : Learning rate\n",
    "      num_iters (int)     : number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (ndarray (n,)) : Updated values of parameters\n",
    "      b (scalar)       : Updated value of parameter\n",
    "      \"\"\"\n",
    "\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)  #avoid modifying global w within function\n",
    "    b = b_in\n",
    "\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db,dj_dw = gradient_function(X, y, w, b)   ##None\n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w = w - alpha * dj_dw               ##None\n",
    "        b = b - alpha * dj_db               ##None\n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion\n",
    "            J_history.append( cost_function(X, y, w, b))\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}   \")\n",
    "\n",
    "    return w, b, J_history #return final w,b and J history for graphing"
   ],
   "id": "f7bfd4779ff6f169",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T20:21:06.145060Z",
     "start_time": "2025-03-18T20:21:01.339088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# initialize parameters\n",
    "#w_init = np.array([ 0.39133535, 18.75376741, -53.36032453, -26.42131618])\n",
    "#initial_w = np.zeros_like(w_init)\n",
    "initial_w = np.zeros((4,))\n",
    "initial_b = 0.\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "alpha = 1e-1\n",
    "# normalize the original features\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "print(f\"X_mu = {X_mu}, \\nX_sigma = {X_sigma}\")\n",
    "#print(f\"Peak to Peak range by column in Raw        X:{np.ptp(X_train,axis=0)}\")\n",
    "#print(f\"Peak to Peak range by column in Normalized X:{np.ptp(X_norm,axis=0)}\")\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist = gradient_descent(X_norm, y_train, initial_w, initial_b,\n",
    "                                                    compute_cost, compute_gradient,\n",
    "                                                    alpha, iterations)\n",
    "print(f\"b,w found by gradient descent: {b_final:0.2f},{w_final} \")\n",
    "m,_ = X_norm.shape\n",
    "for i in range(m):\n",
    "    print(f\"prediction: {np.dot(X_norm[i], w_final) + b_final:0.2f}, target value: {y_train[i]}\")"
   ],
   "id": "c6415504a9c4499",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_mu = [1.42e+03 2.72e+00 1.38e+00 3.84e+01], \n",
      "X_sigma = [411.62   0.65   0.49  25.78]\n",
      "Iteration    0: Cost 57617.03   \n",
      "Iteration 1000: Cost   219.21   \n",
      "Iteration 2000: Cost   219.21   \n",
      "Iteration 3000: Cost   219.21   \n",
      "Iteration 4000: Cost   219.21   \n",
      "Iteration 5000: Cost   219.21   \n",
      "Iteration 6000: Cost   219.21   \n",
      "Iteration 7000: Cost   219.21   \n",
      "Iteration 8000: Cost   219.21   \n",
      "Iteration 9000: Cost   219.21   \n",
      "b,w found by gradient descent: 363.16,[110.56 -21.27 -32.71 -37.97] \n",
      "prediction: 295.18, target value: 300.0\n",
      "prediction: 485.98, target value: 509.8\n",
      "prediction: 389.52, target value: 394.0\n",
      "prediction: 492.15, target value: 540.0\n",
      "prediction: 420.25, target value: 415.0\n",
      "prediction: 222.78, target value: 230.0\n",
      "prediction: 523.42, target value: 560.0\n",
      "prediction: 267.55, target value: 294.0\n",
      "prediction: 685.20, target value: 718.2\n",
      "prediction: 181.75, target value: 200.0\n",
      "prediction: 317.95, target value: 302.0\n",
      "prediction: 479.63, target value: 468.0\n",
      "prediction: 409.95, target value: 374.2\n",
      "prediction: 393.53, target value: 388.0\n",
      "prediction: 286.97, target value: 282.0\n",
      "prediction: 323.28, target value: 311.8\n",
      "prediction: 405.96, target value: 401.0\n",
      "prediction: 436.43, target value: 449.8\n",
      "prediction: 269.84, target value: 301.0\n",
      "prediction: 500.72, target value: 502.0\n",
      "prediction: 328.61, target value: 340.0\n",
      "prediction: 388.16, target value: 400.282\n",
      "prediction: 551.59, target value: 572.0\n",
      "prediction: 241.48, target value: 264.0\n",
      "prediction: 295.46, target value: 304.0\n",
      "prediction: 282.48, target value: 298.0\n",
      "prediction: 217.11, target value: 219.8\n",
      "prediction: 491.17, target value: 490.7\n",
      "prediction: 228.84, target value: 216.96\n",
      "prediction: 341.21, target value: 368.2\n",
      "prediction: 291.36, target value: 280.0\n",
      "prediction: 490.12, target value: 526.87\n",
      "prediction: 238.29, target value: 237.0\n",
      "prediction: 598.53, target value: 562.426\n",
      "prediction: 383.73, target value: 369.8\n",
      "prediction: 452.82, target value: 460.0\n",
      "prediction: 401.27, target value: 374.0\n",
      "prediction: 405.94, target value: 390.0\n",
      "prediction: 172.18, target value: 158.0\n",
      "prediction: 423.58, target value: 426.0\n",
      "prediction: 434.41, target value: 390.0\n",
      "prediction: 277.01, target value: 277.774\n",
      "prediction: 228.84, target value: 216.96\n",
      "prediction: 448.61, target value: 425.8\n",
      "prediction: 489.06, target value: 504.0\n",
      "prediction: 331.76, target value: 329.0\n",
      "prediction: 465.80, target value: 464.0\n",
      "prediction: 221.67, target value: 220.0\n",
      "prediction: 386.72, target value: 358.0\n",
      "prediction: 456.66, target value: 478.0\n",
      "prediction: 370.49, target value: 334.0\n",
      "prediction: 468.87, target value: 426.98\n",
      "prediction: 310.19, target value: 290.0\n",
      "prediction: 426.51, target value: 463.0\n",
      "prediction: 391.78, target value: 390.8\n",
      "prediction: 347.54, target value: 354.0\n",
      "prediction: 339.21, target value: 350.0\n",
      "prediction: 471.59, target value: 460.0\n",
      "prediction: 243.32, target value: 237.0\n",
      "prediction: 297.94, target value: 288.304\n",
      "prediction: 272.87, target value: 282.0\n",
      "prediction: 249.66, target value: 249.0\n",
      "prediction: 297.83, target value: 304.0\n",
      "prediction: 334.93, target value: 332.0\n",
      "prediction: 375.90, target value: 351.8\n",
      "prediction: 288.86, target value: 310.0\n",
      "prediction: 228.84, target value: 216.96\n",
      "prediction: 621.12, target value: 666.336\n",
      "prediction: 352.77, target value: 330.0\n",
      "prediction: 511.13, target value: 480.0\n",
      "prediction: 364.05, target value: 330.3\n",
      "prediction: 363.09, target value: 348.0\n",
      "prediction: 297.83, target value: 304.0\n",
      "prediction: 407.27, target value: 384.0\n",
      "prediction: 288.55, target value: 316.0\n",
      "prediction: 385.90, target value: 430.4\n",
      "prediction: 488.28, target value: 450.0\n",
      "prediction: 260.88, target value: 284.0\n",
      "prediction: 259.05, target value: 275.0\n",
      "prediction: 427.65, target value: 414.0\n",
      "prediction: 238.04, target value: 258.0\n",
      "prediction: 355.57, target value: 378.0\n",
      "prediction: 339.63, target value: 350.0\n",
      "prediction: 390.30, target value: 412.0\n",
      "prediction: 381.62, target value: 373.0\n",
      "prediction: 220.04, target value: 225.0\n",
      "prediction: 434.41, target value: 390.0\n",
      "prediction: 243.33, target value: 267.4\n",
      "prediction: 465.80, target value: 464.0\n",
      "prediction: 185.78, target value: 174.0\n",
      "prediction: 341.22, target value: 340.0\n",
      "prediction: 410.26, target value: 430.0\n",
      "prediction: 445.65, target value: 440.0\n",
      "prediction: 231.87, target value: 216.0\n",
      "prediction: 331.76, target value: 329.0\n",
      "prediction: 409.23, target value: 388.0\n",
      "prediction: 405.94, target value: 390.0\n",
      "prediction: 351.39, target value: 356.0\n",
      "prediction: 274.21, target value: 257.8\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T20:21:14.674138Z",
     "start_time": "2025-03-18T20:21:14.664133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, normalize out example.\n",
    "x_house = np.array([1200, 3, 1, 40])\n",
    "x_house_norm = (x_house - X_mu) / X_sigma\n",
    "print(x_house_norm)\n",
    "x_house_predict = np.dot(x_house_norm, w_final) + b_final\n",
    "print(f\" predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = ${x_house_predict*1000:0.0f}\")"
   ],
   "id": "d2c113fdd53e4f88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.53  0.43 -0.79  0.06]\n",
      " predicted price of a house with 1200 sqft, 3 bedrooms, 1 floor, 40 years old = $318709\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "ef3a2bba5147ce21"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
